{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analyzer For Movie Reviews\n",
    "\n",
    "The scraping process took 2 hours to finish. In the end, I was able to obtain all needed 28 variables for 5043 movies and 4906 posters (998MB), spanning across 100 years in 66 countries. There are 2399 unique director names, and thousands of actors/actresses. Below are the 28 variables:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from tflearn.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_data in module tflearn.datasets.imdb:\n",
      "\n",
      "load_data(path='imdb.pkl', n_words=100000, valid_portion=0.1, maxlen=None, sort_by_len=True)\n",
      "    Loads the dataset\n",
      "    :type path: String\n",
      "    :param path: The path to the dataset (here IMDB)\n",
      "    :type n_words: int\n",
      "    :param n_words: The number of word to keep in the vocabulary.\n",
      "        All extra words are set to unknow (1).\n",
      "    :type valid_portion: float\n",
      "    :param valid_portion: The proportion of the full train set used for\n",
      "        the validation set.\n",
      "    :type maxlen: None or positive int\n",
      "    :param maxlen: the max sequence length we use in the train/valid set.\n",
      "    :type sort_by_len: bool\n",
      "    :name sort_by_len: Sort by the sequence lenght for the train,\n",
      "        valid and test set. This allow faster execution as it cause\n",
      "        less padding per minibatch. Another mechanism must be used to\n",
      "        shuffle the train set at each epoch.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(imdb.load_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#IMDB Dataset loading\n",
    "#load_data in module tflearn.datasets.imdb:\n",
    "#extension is picke which means a bit steam, so it make it easier to convert to other \n",
    "#python objects like list, tuple later \n",
    "#param n_words: The number of word to keep in the vocabulary.All extra words are set to unknow (1).\n",
    "#param valid_portion: The proportion of the full train set used for the validation set.\n",
    "\n",
    "train, test, _ = imdb.load_data(path='imdb.pkl', n_words=10000, valid_portion=0.1)\n",
    "trainX, trainY = train\n",
    "testX, testY = test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "# Sequence padding\n",
    "#function pad_sequences in module tflearn.data_utils:\n",
    "# padding is necessary to have consistency in our input dimensionality\n",
    "trainX = pad_sequences(trainX, maxlen=100, value=0.)\n",
    "testX = pad_sequences(testX, maxlen=100, value=0.)\n",
    "\n",
    "#Now we can convert output to binaries where 0 means Negative and 1 means Positive\n",
    "# Converting labels to binary vectors\n",
    "trainY = to_categorical(trainY, nb_classes=2)\n",
    "testY = to_categorical(testY, nb_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    1    2    1  139    6  130    1    5    6   25  105\n",
      "  4730   40]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0   30  287  142 2216  707 3763   20   68   57   30   37  309\n",
      "    14    4]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0  224    3\n",
      "   371    3    1    4  128   37   16   90   48 1247    8   79  294  913\n",
      "  1709    4]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0   17   10    2 6409\n",
      "    25  325    7 3161    4 2588 1977  176    3   26   50   35   70 1050\n",
      "   395    4]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0   16  104   32  138  152\n",
      "    16   49   17   25   48   89    3   26   16  128   91 1445    7  164\n",
      "    14    4]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0   70  922   87  394   25\n",
      "     3  129  883   53  457   86  879   87   70  297   42   41   86 1752\n",
      "    14   40]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    6  694    7   19  360   19\n",
      "   139   33  893    8 2567  102  760    3 2237    5 6803   96   17   25\n",
      "    10    4]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    2  118   35 6186    5    2\n",
      "   242   10  397    4   14   20    6  456    7    2 1938    7    1    5\n",
      "     1    4]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0  545  445   23    2  889    4 8927   26\n",
      "  2618    4  102   63   30  157  373   24 3163 4598    3 4009    5    1\n",
      "     1    4]]\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "print(trainX[1:10,:])\n",
    "print(trainX[0,:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Network building\n",
    "#function input_data in module tflearn.layers.core:\n",
    "#`input_data` is used as a data entry (placeholder) of a network.\n",
    "#This placeholder will be feeded with data when training\n",
    "# shape: list of `int`. An array or tuple representing input data shape.\n",
    "#            It is required if no placeholder provided. First element should\n",
    "#            be 'None' (representing batch size), if not provided, it will be\n",
    "#            added automatically.\n",
    "net = tflearn.input_data([None, 100])\n",
    "\n",
    "# LAYER-1\n",
    "# input_dim = 10000, since that how many words we loaded from our dataset\n",
    "# output_dim = 128, number of outputs of resulting embedding\n",
    "\n",
    "# LAYER-2\n",
    "net = tflearn.embedding(net, input_dim=10000, output_dim=128)\n",
    "# LSTM = long short term memory\n",
    "# this layer allow us to remember our data from the beginning of the sequences which \n",
    "# allow us to improve our prediction\n",
    "# dropout = 0.8, is the technique to prevent overfitting, which randomly switch on and off different \n",
    "# pathways in our network\n",
    "net = tflearn.lstm(net, 128, dropout=0.8)\n",
    "\n",
    "# LAYER-3: Our next layer is fully connected, this means every neuron in the previous layer is connected to \n",
    "# the every neuron in this layer\n",
    "# Good way of learning non-linear combination\n",
    "# activation: take in vector of input and squash it into vector of output probability between \n",
    "# 0 and 1.\n",
    "net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "\n",
    "# LAYER-4: \n",
    "# adam : perform gradient descent\n",
    "# categorical_crossentropy: find error between predicted output and original output \n",
    "net = tflearn.regression(net, optimizer='adam', learning_rate=0.001, loss='categorical_crossentropy',shuffle_batches=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-61b8d9ace344>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard_verbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# Initialize with tflearn Deep Neural Network Model.\n",
    "# tensorboard_verbose: `int`. Summary verbose level, it accepts\n",
    "#          different levels of tensorboard logs:\n",
    "#          0: Loss, Accuracy (Best Speed).\n",
    "#          1: Loss, Accuracy, Gradients.\n",
    "#          2: Loss, Accuracy, Gradients, Weights.\n",
    "#          3: Loss, Accuracy, Gradients, Weights, Activations, Sparsity.\n",
    "#              (Best visualization)\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "model.fit(trainX, trainY, validation_set=(testX, testY), show_metric=True,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DNN in module tflearn.models.dnn:\n",
      "\n",
      "class DNN(builtins.object)\n",
      " |  Deep Neural Network Model.\n",
      " |  \n",
      " |  Arguments:\n",
      " |      network: `Tensor`. Neural network to be used.\n",
      " |      tensorboard_verbose: `int`. Summary verbose level, it accepts\n",
      " |          different levels of tensorboard logs:\n",
      " |          ```python\n",
      " |          0: Loss, Accuracy (Best Speed).\n",
      " |          1: Loss, Accuracy, Gradients.\n",
      " |          2: Loss, Accuracy, Gradients, Weights.\n",
      " |          3: Loss, Accuracy, Gradients, Weights, Activations, Sparsity.\n",
      " |              (Best visualization)\n",
      " |          ```\n",
      " |      tensorboard_dir: `str`. Directory to store tensorboard logs.\n",
      " |          Default: \"/tmp/tflearn_logs/\"\n",
      " |      checkpoint_path: `str`. Path to store model checkpoints. If None,\n",
      " |          no model checkpoint will be saved. Default: None.\n",
      " |      max_checkpoints: `int` or None. Maximum amount of checkpoints. If\n",
      " |          None, no limit. Default: None.\n",
      " |      session: `Session`. A session for running ops. If None, a new one will\n",
      " |          be created. Note: When providing a session, variables must have been\n",
      " |          initialized already, otherwise an error will be raised.\n",
      " |  \n",
      " |  Attributes:\n",
      " |      trainer: `Trainer`. Handle model training.\n",
      " |      predictor: `Predictor`. Handle model prediction.\n",
      " |      session: `Session`. The current model session.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, network, clip_gradients=5.0, tensorboard_verbose=0, tensorboard_dir='/tmp/tflearn_logs/', checkpoint_path=None, max_checkpoints=None, session=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  evaluate(self, X, Y, batch_size=128)\n",
      " |      Evaluate.\n",
      " |      \n",
      " |      Evaluate model metric(s) on given samples.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          X: array, `list` of array (if multiple inputs) or `dict`\n",
      " |              (with inputs layer name as keys). Data to feed to train\n",
      " |              model.\n",
      " |          Y: array, `list` of array (if multiple inputs) or `dict`\n",
      " |              (with estimators layer name as keys). Targets (Labels) to\n",
      " |              feed to train model. Usually set as the next element of a\n",
      " |              sequence, i.e. for x[0] => y[0] = x[1].\n",
      " |          batch_size: `int`. The batch size. Default: 128.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The metric(s) score.\n",
      " |  \n",
      " |  fit(self, X_inputs, Y_targets, n_epoch=10, validation_set=None, show_metric=False, batch_size=None, shuffle=None, snapshot_epoch=True, snapshot_step=None, excl_trainops=None, run_id=None)\n",
      " |      Fit.\n",
      " |      \n",
      " |      Train model, feeding X_inputs and Y_targets to the network.\n",
      " |      \n",
      " |      NOTE: When not feeding dicts, data assignations is made by\n",
      " |          input/estimator layers creation order (For example, the second\n",
      " |          input layer created will be feeded by the second value of\n",
      " |          X_inputs list).\n",
      " |      \n",
      " |      Examples:\n",
      " |          ```python\n",
      " |          model.fit(X, Y) # Single input and output\n",
      " |          model.fit({'input1': X}, {'output1': Y}) # Single input and output\n",
      " |          model.fit([X1, X2], Y) # Mutliple inputs, Single output\n",
      " |      \n",
      " |          # validate with X_val and [Y1_val, Y2_val]\n",
      " |          model.fit(X, [Y1, Y2], validation_set=(X_val, [Y1_val, Y2_val]))\n",
      " |          # 10% of training data used for validation\n",
      " |          model.fit(X, Y, validation_set=0.1)\n",
      " |          ```\n",
      " |      \n",
      " |      Arguments:\n",
      " |          X_inputs: array, `list` of array (if multiple inputs) or `dict`\n",
      " |              (with inputs layer name as keys). Data to feed to train\n",
      " |              model.\n",
      " |          Y_targets: array, `list` of array (if multiple inputs) or `dict`\n",
      " |              (with estimators layer name as keys). Targets (Labels) to\n",
      " |              feed to train model.\n",
      " |          n_epoch: `int`. Number of epoch to run. Default: None.\n",
      " |          validation_set: `tuple`. Represents data used for validation.\n",
      " |              `tuple` holds data and targets (provided as same type as\n",
      " |              X_inputs and Y_targets). Additionally, it also accepts\n",
      " |              `float` (<1) to performs a data split over training data.\n",
      " |          show_metric: `bool`. Display or not accuracy at every step.\n",
      " |          batch_size: `int` or None. If `int`, overrides all network\n",
      " |              estimators 'batch_size' by this value.\n",
      " |          shuffle: `bool` or None. If `bool`, overrides all network\n",
      " |              estimators 'shuffle' by this value.\n",
      " |          snapshot_epoch: `bool`. If True, it will snapshot model at the end\n",
      " |              of every epoch. (Snapshot a model will evaluate this model\n",
      " |              on validation set, as well as create a checkpoint if\n",
      " |              'checkpoint_path' specified).\n",
      " |          snapshot_step: `int` or None. If `int`, it will snapshot model\n",
      " |              every 'snapshot_step' steps.\n",
      " |          excl_trainops: `list` of `TrainOp`. A list of train ops to\n",
      " |              exclude from training process (TrainOps can be retrieve\n",
      " |              through `tf.get_collection_ref(tf.GraphKeys.TRAIN_OPS)`).\n",
      " |          run_id: `str`. Give a name for this run. (Useful for Tensorboard).\n",
      " |  \n",
      " |  get_weights(self, weight_tensor)\n",
      " |      Get Weights.\n",
      " |      \n",
      " |      Get a variable weights.\n",
      " |      \n",
      " |      Examples:\n",
      " |          ```\n",
      " |          dnn = DNNTrainer(...)\n",
      " |          w = dnn.get_weights(denselayer.W) # get a dense layer weights\n",
      " |          w = dnn.get_weights(convlayer.b) # get a conv layer biases\n",
      " |          ```\n",
      " |      \n",
      " |      Arguments:\n",
      " |          weight_tensor: `Tensor`. A Variable.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `np.array`. The provided variable weights.\n",
      " |  \n",
      " |  load(self, model_file)\n",
      " |      Load.\n",
      " |      \n",
      " |      Restore model weights.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          model_file: `str`. Model path.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict.\n",
      " |      \n",
      " |      Model prediction for given input data.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          X: array, `list` of array (if multiple inputs) or `dict`\n",
      " |              (with inputs layer name as keys). Data to feed for prediction.\n",
      " |      \n",
      " |      Returns:\n",
      " |          array or `list` of array. The predicted value.\n",
      " |  \n",
      " |  save(self, model_file)\n",
      " |      Save.\n",
      " |      \n",
      " |      Save model weights.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          model_file: `str`. Model path.\n",
      " |  \n",
      " |  set_weights(self, tensor, weights)\n",
      " |      Set Weights.\n",
      " |      \n",
      " |      Assign a tensor variable a given value.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          tensor: `Tensor`. The tensor variable to assign value.\n",
      " |          weights: The value to be assigned.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tflearn.DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictionsY = model.predict(testX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(testX[:,1], testX[:,2])\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Imdb Film Dataset')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citation\n",
    "\n",
    "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
    "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
    "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
    "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
    "  month     = {June},\n",
    "  year      = {2011},\n",
    "  address   = {Portland, Oregon, USA},\n",
    "  publisher = {Association for Computational Linguistics},\n",
    "  pages     = {142--150},\n",
    "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
    "}\n",
    "\n",
    "# References\n",
    "\n",
    "Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\n",
    "David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\n",
    "636-659."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
