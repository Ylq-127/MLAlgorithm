{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analyzer For Movie Reviews\n",
    "\n",
    "The scraping process took 2 hours to finish. In the end, I was able to obtain all needed 28 variables for 5043 movies and 4906 posters (998MB), spanning across 100 years in 66 countries. There are 2399 unique director names, and thousands of actors/actresses. Below are the 28 variables:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from tflearn.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_data in module tflearn.datasets.imdb:\n",
      "\n",
      "load_data(path='imdb.pkl', n_words=100000, valid_portion=0.1, maxlen=None, sort_by_len=True)\n",
      "    Loads the dataset\n",
      "    :type path: String\n",
      "    :param path: The path to the dataset (here IMDB)\n",
      "    :type n_words: int\n",
      "    :param n_words: The number of word to keep in the vocabulary.\n",
      "        All extra words are set to unknow (1).\n",
      "    :type valid_portion: float\n",
      "    :param valid_portion: The proportion of the full train set used for\n",
      "        the validation set.\n",
      "    :type maxlen: None or positive int\n",
      "    :param maxlen: the max sequence length we use in the train/valid set.\n",
      "    :type sort_by_len: bool\n",
      "    :name sort_by_len: Sort by the sequence lenght for the train,\n",
      "        valid and test set. This allow faster execution as it cause\n",
      "        less padding per minibatch. Another mechanism must be used to\n",
      "        shuffle the train set at each epoch.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(imdb.load_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#IMDB Dataset loading\n",
    "#load_data in module tflearn.datasets.imdb:\n",
    "#extension is picke which means a bit steam, so it make it easier to convert to other \n",
    "#python objects like list, tuple later \n",
    "#param n_words: The number of word to keep in the vocabulary.All extra words are set to unknow (1).\n",
    "#param valid_portion: The proportion of the full train set used for the validation set.\n",
    "\n",
    "train, test, _ = imdb.load_data(path='imdb.pkl', n_words=10000, valid_portion=0.1)\n",
    "trainX, trainY = train\n",
    "testX, testY = test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "# Sequence padding\n",
    "#function pad_sequences in module tflearn.data_utils:\n",
    "# padding is necessary to have consistency in our input dimensionality\n",
    "trainX = pad_sequences(trainX, maxlen=100, value=0.)\n",
    "testX = pad_sequences(testX, maxlen=100, value=0.)\n",
    "\n",
    "#Now we can convert output to binaries where 0 means Negative and 1 means Positive\n",
    "# Converting labels to binary vectors\n",
    "trainY = to_categorical(trainY, nb_classes=2)\n",
    "testY = to_categorical(testY, nb_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0   16  586   32  885   17   39   68   31 2994 2389\n",
      "   328    4]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    1    2    1  139    6  130    1    5    6   25  105\n",
      "  4730   40]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0  224    3\n",
      "   371    3    1    4  128   37   16   90   48 1247    8   79  294  913\n",
      "  1709    4]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0   17   10    2 6409\n",
      "    25  325    7 3161    4 2588 1977  176    3   26   50   35   70 1050\n",
      "   395    4]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0   16  104   32  138  152\n",
      "    16   49   17   25   48   89    3   26   16  128   91 1445    7  164\n",
      "    14    4]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0   70  922   87  394   25\n",
      "     3  129  883   53  457   86  879   87   70  297   42   41   86 1752\n",
      "    14   40]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    2  118   35 6186    5    2\n",
      "   242   10  397    4   14   20    6  456    7    2 1938    7    1    5\n",
      "     1    4]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    6  694    7   19  360   19\n",
      "   139   33  893    8 2567  102  760    3 2237    5 6803   96   17   25\n",
      "    10    4]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0  545  445   23    2  889    4 8927   26\n",
      "  2618    4  102   63   30  157  373   24 3163 4598    3 4009    5    1\n",
      "     1    4]]\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "print(trainX[1:10,:])\n",
    "print(trainX[0,:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Network building\n",
    "#function input_data in module tflearn.layers.core:\n",
    "#`input_data` is used as a data entry (placeholder) of a network.\n",
    "#This placeholder will be feeded with data when training\n",
    "# shape: list of `int`. An array or tuple representing input data shape.\n",
    "#            It is required if no placeholder provided. First element should\n",
    "#            be 'None' (representing batch size), if not provided, it will be\n",
    "#            added automatically.\n",
    "net = tflearn.input_data([None, 100])\n",
    "\n",
    "# LAYER-1\n",
    "# input_dim = 10000, since that how many words we loaded from our dataset\n",
    "# output_dim = 128, number of outputs of resulting embedding\n",
    "\n",
    "# LAYER-2\n",
    "net = tflearn.embedding(net, input_dim=10000, output_dim=128)\n",
    "# LSTM = long short term memory\n",
    "# this layer allow us to remember our data from the beginning of the sequences which \n",
    "# allow us to improve our prediction\n",
    "# dropout = 0.8, is the technique to prevent overfitting, which randomly switch on and off different \n",
    "# pathways in our network\n",
    "net = tflearn.lstm(net, 128, dropout=0.8)\n",
    "\n",
    "# LAYER-3: Our next layer is fully connected, this means every neuron in the previous layer is connected to \n",
    "# the every neuron in this layer\n",
    "# Good way of learning non-linear combination\n",
    "# activation: take in vector of input and squash it into vector of output probability between \n",
    "# 0 and 1.\n",
    "net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "\n",
    "# LAYER-4: \n",
    "# adam : perform gradient descent\n",
    "# categorical_crossentropy: find error between predicted output and original output \n",
    "net = tflearn.regression(net, optimizer='adam', learning_rate=0.001, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/amanullahtariq/anaconda3/lib/python3.5/site-packages/tflearn/summaries.py:46 in get_summary.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From /home/amanullahtariq/anaconda3/lib/python3.5/site-packages/tflearn/summaries.py:46 in get_summary.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From /home/amanullahtariq/anaconda3/lib/python3.5/site-packages/tflearn/helpers/trainer.py:766 in create_summaries.: merge_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge.\n",
      "WARNING:tensorflow:From /home/amanullahtariq/anaconda3/lib/python3.5/site-packages/tflearn/summaries.py:46 in get_summary.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From /home/amanullahtariq/anaconda3/lib/python3.5/site-packages/tflearn/summaries.py:46 in get_summary.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# Initialize with tflearn Deep Neural Network Model.\n",
    "# tensorboard_verbose: `int`. Summary verbose level, it accepts\n",
    "#          different levels of tensorboard logs:\n",
    "#          0: Loss, Accuracy (Best Speed).\n",
    "#          1: Loss, Accuracy, Gradients.\n",
    "#          2: Loss, Accuracy, Gradients, Weights.\n",
    "#          3: Loss, Accuracy, Gradients, Weights, Activations, Sparsity.\n",
    "#              (Best visualization)\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "model.fit(trainX, trainY, validation_set=(testX, testY), show_metric=True,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(tflearn.DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictionsY = model.predict(testX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(testX[:,1], testX[:,2])\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Imdb Film Dataset')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citation\n",
    "\n",
    "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
    "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
    "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
    "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
    "  month     = {June},\n",
    "  year      = {2011},\n",
    "  address   = {Portland, Oregon, USA},\n",
    "  publisher = {Association for Computational Linguistics},\n",
    "  pages     = {142--150},\n",
    "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
    "}\n",
    "\n",
    "# References\n",
    "\n",
    "Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\n",
    "David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\n",
    "636-659."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
